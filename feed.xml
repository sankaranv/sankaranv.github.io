<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sankaranv.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sankaranv.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-10T00:26:23+00:00</updated><id>https://sankaranv.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Sankaran Vaidyanathan. </subtitle><entry><title type="html">Type and Token Causality</title><link href="https://sankaranv.github.io/blog/type_and_token" rel="alternate" type="text/html" title="Type and Token Causality"/><published>2024-04-20T00:01:00+00:00</published><updated>2024-04-20T00:01:00+00:00</updated><id>https://sankaranv.github.io/blog/type_and_token</id><content type="html" xml:base="https://sankaranv.github.io/blog/type_and_token"><![CDATA[<p>Philosophers have typically distinguished two notions of causality: <em>type causality</em>, sometimes called general causality, makes general statements about causal relationships between entities in the world, but <em>actual causality</em>, sometimes called token causality or specific causality, makes statements about particular events. For example, type causality is used to make conclusions such as “smoking causes cancer”, and actual causality is used to make conclusions such as “the fact that this person smoked one pack of cigarettes a day for 30 years caused him to develop the tumor”. The vast majority of work in causal inference has dealt with type causation, and its utility mainly lies in tasks that require making predictions. Knowledge of general causal relationships between entities in the world (or in a model) allows one to make inferences such as “if you smoke one pack of cigarettes a day, you are likely to get lung cancer”. Prediction is the focus of many efforts in machine learning, and type causation is often used in applications where we are looking forward. In contrast, actual causality is mostly concerned with applications that are looking backwards, and identifying causes that have already occurred for events that have already happened.</p> <p>In particular, actual causality is a critical component of blame and responsibility assignment, explanation, and safety assessment. A statement like “the accident was caused by the faulty brakes in the car, and not the pouring rain or the blood alcohol level of the driver” falls within the domain of actual causality. The intuitions behind many previous works in actual causality come from examples in law, where we often know the relevant facts but still need to establish causality. Both type and actual causality have important applications and are intertwined, but the latter has been studied far less in the various communities concerned with empirical research in causal inference. Reasoning about actual causation is a principled and promising direction for research in AI, particularly for explanation, safety, and robustness. At any given instant there are likely to be many events occurring in the environment that are distracting and irrelevant to the current state of an agent. Disregarding these can aid in training more robust agents and generating more precise explanations or safety assessments, but type causality does not provide the capability to prune out these events on a case-by-case basis. Additionally, it is possible to make counterintuitive and incorrect inferences if applying type causal statements to actual causal queries. In previous work, actual causality has been used to formalize problems in AI such as explanation, blame and responsibility, and assessment of harm. However, not much progress in this field has been made outside of the philosophy literature, and the ability to accurately and tractably infer actual causation is essential for it to be applied to real AI systems.</p> <h2 id="where-type-and-token-causation-disagree">Where type and token causation disagree</h2> <p>In general, statements about type causation cannot be directly applied to making inferences about actual causation. In particular, simple counterfactual reasoning with a causal model that was trained to identify type causes can give counterintuitive results when applied to actual causation. This is mostly illustrated in the philosophy literature using examples, such as throwing rocks at a bottle or a gang of shooters firing at a single target, but the implications of these examples are more generally applicable to scenarios in ML and RL environments.</p> <p>The most common problems where type causal inferences fail to correctly establish actual causality are <em>preemption</em>, <em>overdetermination</em>, and <em>normality</em>. Preemption occurs when there are multiple causes of an outcome but one always precedes the other. In these cases, the type causal relationship between the slower cause and the outcome will appear or disappear based on the specific assignment to the faster cause. Rock throwing is a classic example of preemption. Overdetermination occurs when there are multiple simultaneous causes for the same outcome, and it is not clear whether any one of them is necessary. For example, if there are three different events and any one of them was sufficient to cause the outcome, we would not see a causal effect between any single event and the outcome because the other two would always guarantee that the outcome is unchanged. Forest fire and gang shooting are classic examples of this. Normality problems occur when there are multiple candidate causes but one of them is clearly less plausible than the other. The Queen of England problem and the agent and obstacle problem are examples of this.</p> <p>One reason for the appearance of these examples is because while many variables can have general causal relationships with a given outcome variable, in a specific observed context only a select few might actually be responsible for the outcome that was observed. Depending on the nature of the model or world, some type causal relationships actually appear only in certain instances. In general, the problem of inferring causation in the presence of context-specific independences is inextricably tied to actual causality. For example, if an agent is trying to push a block and there are obstacles present in the environment, all the obstacles have causal relationships with the block being pushed but they only become active when they are at a certain level of proximity to the block.</p>]]></content><author><name></name></author><category term="causality"/><summary type="html"><![CDATA[Philosophers have typically distinguished two notions of causality: type causality, sometimes called general causality, makes general statements about causal relationships between entities in the world, but actual causality, sometimes called token causality or specific causality, makes statements about particular events. For example, type causality is used to make conclusions such as “smoking causes cancer”, and actual causality is used to make conclusions such as “the fact that this person smoked one pack of cigarettes a day for 30 years caused him to develop the tumor”. The vast majority of work in causal inference has dealt with type causation, and its utility mainly lies in tasks that require making predictions. Knowledge of general causal relationships between entities in the world (or in a model) allows one to make inferences such as “if you smoke one pack of cigarettes a day, you are likely to get lung cancer”. Prediction is the focus of many efforts in machine learning, and type causation is often used in applications where we are looking forward. In contrast, actual causality is mostly concerned with applications that are looking backwards, and identifying causes that have already occurred for events that have already happened.]]></summary></entry></feed>